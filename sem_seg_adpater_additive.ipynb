{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 275/275 [05:24<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.1196\n",
      "Validation Loss: 0.9803\n",
      "Model and adapters saved to /projectnb/compvision/jteja/Lexicon3D/lexicon3d/runs/dino+clip/best_model_additive_adapter.pth\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 275/275 [05:51<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.8717\n",
      "Validation Loss: 0.9228\n",
      "Model and adapters saved to /projectnb/compvision/jteja/Lexicon3D/lexicon3d/runs/dino+clip/best_model_additive_adapter.pth\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 275/275 [05:53<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.8125\n",
      "Validation Loss: 0.8969\n",
      "Model and adapters saved to /projectnb/compvision/jteja/Lexicon3D/lexicon3d/runs/dino+clip/best_model_additive_adapter.pth\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 275/275 [05:49<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.7633\n",
      "Validation Loss: 0.8873\n",
      "Model and adapters saved to /projectnb/compvision/jteja/Lexicon3D/lexicon3d/runs/dino+clip/best_model_additive_adapter.pth\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 275/275 [05:43<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 0.7308\n",
      "Validation Loss: 0.8809\n",
      "Model and adapters saved to /projectnb/compvision/jteja/Lexicon3D/lexicon3d/runs/dino+clip/best_model_additive_adapter.pth\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 275/275 [05:40<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Training Loss: 0.7006\n",
      "Validation Loss: 0.8811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 275/275 [05:45<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Training Loss: 0.6719\n",
      "Validation Loss: 0.8799\n",
      "Model and adapters saved to /projectnb/compvision/jteja/Lexicon3D/lexicon3d/runs/dino+clip/best_model_additive_adapter.pth\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 275/275 [05:34<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Training Loss: 0.6505\n",
      "Validation Loss: 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 275/275 [05:32<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Training Loss: 0.6272\n",
      "Validation Loss: 0.8853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 275/275 [05:57<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training Loss: 0.6096\n",
      "Validation Loss: 0.8934\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set up the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Adapter with normalization\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, input_dim, target_dim, hidden_dim):\n",
    "        super(Adapter, self).__init__()\n",
    "        self.down_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.non_linearity = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.up_proj = nn.Linear(hidden_dim, target_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down_proj(x)\n",
    "        x = self.non_linearity(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.up_proj(x)\n",
    "        return x  # No residual connection for dimension-changing projection\n",
    "\n",
    "# Add features with normalization\n",
    "def add_features(clip_embeddings, dino_embeddings, clip_adapter, dino_adapter):\n",
    "    # Ensure embeddings are in float32\n",
    "    clip_embeddings = clip_embeddings.to(dtype=torch.float32)\n",
    "    dino_embeddings = dino_embeddings.to(dtype=torch.float32)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    clip_embeddings = clip_embeddings / (clip_embeddings.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "    dino_embeddings = dino_embeddings / (dino_embeddings.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "\n",
    "    # Apply adapters\n",
    "    adapted_clip = clip_adapter(clip_embeddings)\n",
    "    adapted_dino = dino_adapter(dino_embeddings)\n",
    "\n",
    "    # Combine adapted features\n",
    "    combined_features = adapted_clip + adapted_dino\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class AdditiveScanNetDataset(Dataset):\n",
    "    def __init__(self, scene_ids_file, data_dir, clip_embedding_dir, dino_embedding_dir):\n",
    "        self.scene_ids = self._load_scene_ids(scene_ids_file)\n",
    "        self.data_dir = data_dir\n",
    "        self.clip_embedding_dir = clip_embedding_dir\n",
    "        self.dino_embedding_dir = dino_embedding_dir\n",
    "\n",
    "    def _load_scene_ids(self, scene_ids_file):\n",
    "        with open(scene_ids_file, 'r') as f:\n",
    "            return f.read().splitlines()\n",
    "\n",
    "    def _load_scene_data(self, scene_id):\n",
    "        scene_data = torch.load(os.path.join(self.data_dir, f'{scene_id}_vh_clean_2.pth'))\n",
    "        labels = scene_data[2]\n",
    "        coordinates = scene_data[0]\n",
    "\n",
    "        clip_data = torch.load(os.path.join(self.clip_embedding_dir, f'{scene_id}.pt'))\n",
    "        dino_data = torch.load(os.path.join(self.dino_embedding_dir, f'{scene_id}.pt'))\n",
    "\n",
    "        clip_embeddings, clip_mask = clip_data['feat'], clip_data['mask_full']\n",
    "        dino_embeddings, dino_mask = dino_data['feat'], dino_data['mask_full']\n",
    "\n",
    "        # Ensure masks align\n",
    "        if not torch.equal(clip_mask, dino_mask):\n",
    "            return None\n",
    "        common_mask = clip_mask & dino_mask\n",
    "        filtered_labels = labels[common_mask]\n",
    "\n",
    "        # Replace label 255 with 20\n",
    "        filtered_labels[filtered_labels == 255] = 20\n",
    "\n",
    "        return (\n",
    "            clip_embeddings, \n",
    "            dino_embeddings,\n",
    "            coordinates[common_mask],\n",
    "            filtered_labels\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scene_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        result = None\n",
    "        while result is None:\n",
    "            scene_id = self.scene_ids[idx]\n",
    "            result = self._load_scene_data(scene_id)\n",
    "            idx = random.randint(0, len(self.scene_ids) - 1) if result is None else idx\n",
    "\n",
    "        return *result, scene_id\n",
    "\n",
    "# Collate function\n",
    "def custom_collate(batch):\n",
    "    return tuple(map(list, zip(*batch)))\n",
    "\n",
    "# Segmentation model\n",
    "class SimpleSegmentationModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SimpleSegmentationModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x.float())\n",
    "        return torch.log(self.softmax(x))  # Log probabilities for NLLLoss\n",
    "\n",
    "# Save the model and adapters\n",
    "def save_model_and_adapters(model, clip_adapter, dino_adapter, filepath):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'clip_adapter_state_dict': clip_adapter.state_dict(),\n",
    "        'dino_adapter_state_dict': dino_adapter.state_dict(),\n",
    "    }, filepath)\n",
    "    print(f\"Model and adapters saved to {filepath}\")\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, clip_adapter, dino_adapter, num_epochs=10, patience=3):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for clip_list, dino_list, _, label_list, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            for clip, dino, labels in zip(clip_list, dino_list, label_list):\n",
    "                clip = clip.to(device)\n",
    "                dino = dino.to(device)\n",
    "                labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                combined_features = add_features(clip, dino, clip_adapter, dino_adapter)\n",
    "                outputs = model(combined_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for clip_list, dino_list, _, label_list, _ in val_loader:\n",
    "                for clip, dino, labels in zip(clip_list, dino_list, label_list):\n",
    "                    clip = clip.to(device)\n",
    "                    dino = dino.to(device)\n",
    "                    labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "                    combined_features = add_features(clip, dino, clip_adapter, dino_adapter)\n",
    "                    outputs = model(combined_features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            save_model_and_adapters(model, clip_adapter, dino_adapter, \"/projectnb/compvision/jteja/Lexicon3D/lexicon3d/runs/dino+clip/best_model_additive_adapter.pth\")\n",
    "            print(\"Model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "clip_dim = 3072\n",
    "dino_dim = 1024\n",
    "hidden_dim = 512\n",
    "target_dim = 1024\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "batch_size = 1\n",
    "\n",
    "# Initialize components\n",
    "clip_adapter = Adapter(clip_dim, target_dim, hidden_dim).to(device)\n",
    "dino_adapter = Adapter(dino_dim, target_dim, hidden_dim).to(device)\n",
    "model = SimpleSegmentationModel(target_dim, num_classes=21).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(\n",
    "    list(model.parameters()) + list(clip_adapter.parameters()) + list(dino_adapter.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# Paths to the data\n",
    "train_scene_ids_file = '/projectnb/compvision/charoori/openscene/data/scannet_3d/scannetv2_train_filtered1.txt'\n",
    "val_scene_ids_file = '/projectnb/compvision/charoori/openscene/data/scannet_3d/scannetv2_val_filtered1.txt'\n",
    "train_data_dir = '/projectnb/compvision/charoori/openscene/data/scannet_3d/train'\n",
    "val_data_dir = '/projectnb/compvision/charoori/openscene/data/scannet_3d/val'\n",
    "clip_embedding_dir = '/projectnb/compvision/jteja/Lexicon3D/lexicon3d/clip/clip_features'\n",
    "dino_embedding_dir = '/projectnb/compvision/charoori/Lexicon3D/lexicon3d/dataset/lexicon3d/dinov2_v2/dinov2_features'\n",
    "\n",
    "# Dataset and Dataloader\n",
    "train_dataset = AdditiveScanNetDataset(\n",
    "    train_scene_ids_file, train_data_dir, clip_embedding_dir, dino_embedding_dir\n",
    ")\n",
    "val_dataset = AdditiveScanNetDataset(\n",
    "    val_scene_ids_file, val_data_dir, clip_embedding_dir, dino_embedding_dir\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=custom_collate)\n",
    "\n",
    "# Train\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, clip_adapter, dino_adapter, num_epochs=num_epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and visualization\n",
    "def validate_and_visualize(model, val_loader, clip_adapter, dino_adapter):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    intersection_sum = np.zeros(20)\n",
    "    union_sum = np.zeros(20)\n",
    "    scene_data = {}\n",
    "\n",
    "    for clip_list, dino_list, coordinates_list, labels_list, scene_ids in tqdm(val_loader):\n",
    "        for clip, dino, coordinates, labels, scene_id in zip(clip_list, dino_list, coordinates_list, labels_list, scene_ids):\n",
    "            clip = clip.to(device)\n",
    "            dino = dino.to(device)\n",
    "            labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "            combined_features = add_features(clip, dino, clip_adapter, dino_adapter)\n",
    "            outputs = model(combined_features)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.numel()\n",
    "\n",
    "            pred_np = predicted.cpu().numpy()\n",
    "            labels_np = labels.cpu().numpy()\n",
    "\n",
    "            for class_idx in range(20):\n",
    "                intersection = np.logical_and(pred_np == class_idx, labels_np == class_idx).sum()\n",
    "                union = np.logical_or(pred_np == class_idx, labels_np == class_idx).sum()\n",
    "                intersection_sum[class_idx] += intersection\n",
    "                union_sum[class_idx] += union\n",
    "\n",
    "            scene_data[scene_id] = {\n",
    "                \"coordinates\": coordinates,\n",
    "                \"labels\": labels_np,\n",
    "                \"predictions\": pred_np\n",
    "            }\n",
    "\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "    miou = np.mean(intersection_sum / (union_sum + 1e-6))\n",
    "    print(f\"Accuracy: {accuracy:.2f}%, mIoU: {miou:.4f}\")\n",
    "\n",
    "    # Visualization\n",
    "    selected_scene = list(scene_data.keys())[0]\n",
    "    data = scene_data[selected_scene]\n",
    "    x, y, z = data[\"coordinates\"].T\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]])\n",
    "    fig.add_trace(go.Scatter3d(x=x, y=y, z=z, mode='markers', marker=dict(size=2, color=data[\"labels\"], opacity=0.8)), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter3d(x=x, y=y, z=z, mode='markers', marker=dict(size=2, color=data[\"predictions\"], opacity=0.8)), row=1, col=2)\n",
    "    fig.update_layout(title=f\"Scene {selected_scene}: Original vs Predicted\", showlegend=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/62 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1018.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 446.12 MiB is free. Including non-PyTorch memory, this process has 43.97 GiB memory in use. Of the allocated memory 42.03 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Validate and visualize\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m validate_and_visualize(model, val_loader, clip_adapter, dino_adapter)\n",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m, in \u001b[0;36mvalidate_and_visualize\u001b[0;34m(model, val_loader, clip_adapter, dino_adapter)\u001b[0m\n\u001b[1;32m     13\u001b[0m dino \u001b[39m=\u001b[39m dino\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(labels, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 16\u001b[0m combined_features \u001b[39m=\u001b[39m add_features(clip, dino, clip_adapter, dino_adapter)\n\u001b[1;32m     17\u001b[0m outputs \u001b[39m=\u001b[39m model(combined_features)\n\u001b[1;32m     19\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs, \u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m, in \u001b[0;36madd_features\u001b[0;34m(clip_embeddings, dino_embeddings, clip_adapter, dino_adapter)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_features\u001b[39m(clip_embeddings, dino_embeddings, clip_adapter, dino_adapter):\n\u001b[1;32m     44\u001b[0m     \u001b[39m# Ensure embeddings are in float32\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     clip_embeddings \u001b[39m=\u001b[39m clip_embeddings\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m     46\u001b[0m     dino_embeddings \u001b[39m=\u001b[39m dino_embeddings\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     48\u001b[0m     \u001b[39m# Normalize embeddings\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1018.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 446.12 MiB is free. Including non-PyTorch memory, this process has 43.97 GiB memory in use. Of the allocated memory 42.03 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Validate and visualize\n",
    "validate_and_visualize(model, val_loader, clip_adapter, dino_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
